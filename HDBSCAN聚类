import pandas as pd
import numpy as np
import hdbscan
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler
from sklearn.decomposition import PCA
from sklearn.metrics import calinski_harabasz_score, davies_bouldin_score, silhouette_score
from mpl_toolkits.mplot3d import Axes3D  # 导入3D图形库
import umap

# 读取数据
file_path = r'xxxx'
df = pd.read_csv(file_path)

# 选择需要进行聚类的列
columns_to_cluster = [
    'bus_trip_count', 'subway_trip_count', 'subway_trip_distance', 'subway_trip_cost',
    'taxi_trip_count', 'taxi_trip_distance',
    'bike_trip_count', 'bike_trip_duration', 'ferry_trip_count'
]
data = df[columns_to_cluster].values

# 1. 数据归一化
scaler = MinMaxScaler()
data_normalized = scaler.fit_transform(data)

# 2. 使用 HDBSCAN 进行聚类
hdbscan_model = hdbscan.HDBSCAN(min_cluster_size=50, min_samples=80, cluster_selection_method='leaf')
clusters = hdbscan_model.fit_predict(data_normalized)

# 3. 将聚类结果添加到原始数据中
df['Cluster'] = clusters

# 4. 使用 UMAP 降维到 3 维
umap_model = umap.UMAP(n_components=3)
umap_result = umap_model.fit_transform(data_normalized)

# 5. 创建 UMAP 数据框并添加聚类结果
umap_df = pd.DataFrame(umap_result, columns=['umap_1', 'umap_2', 'umap_3'])
umap_df['cluster'] = clusters

# 6. 可视化聚类结果（3D散点图）
fig = plt.figure(figsize=(10, 8))
ax = fig.add_subplot(111, projection='3d')

# 使用不同的颜色标记不同簇
scatter = ax.scatter(umap_df['umap_1'], umap_df['umap_2'], umap_df['umap_3'],
                     c=umap_df['cluster'], cmap='viridis', alpha=0.7, s=30)

# 设置图表标题和坐标轴标签
ax.set_title("3D UMAP Visualization (All Features Weighted 1.0)", fontsize=16)
ax.set_xlabel("UMAP Component 1")
ax.set_ylabel("UMAP Component 2")
ax.set_zlabel("UMAP Component 3")

# 显示颜色条
plt.colorbar(scatter, ax=ax, label='Cluster')

# 展示图形
plt.show()


# 输出部分结果查看
print(df[['bus_trip_count', 'subway_trip_count', 'subway_trip_distance',
          'taxi_trip_count', 'taxi_trip_distance',
          'bike_trip_count', 'bike_trip_duration', 'ferry_trip_count', 'Cluster']].head())

# 计算聚类评估指标
silhouette = silhouette_score(data_normalized, clusters)
db_index = davies_bouldin_score(data_normalized, clusters)
ch_index = calinski_harabasz_score(data_normalized, clusters)

print(f"Silhouette Score: {silhouette:.4f}")
print(f"Davies-Bouldin Index: {db_index:.4f} (越小越好)")
print(f"Calinski-Harabasz Index: {ch_index:.4f} (越大越好)")

# 保存聚类结果到新的文件
output_file_path = r'C:\Users\777\Desktop\研究生\hzy毕设\数据\随申行\第五次汇报\非常用用户\30天\聚类结果2.csv'
df.to_csv(output_file_path, index=False, encoding='utf-8-sig')
print(f"聚类结果已保存到 {output_file_path}")

# 计算每个簇的80%中位值和样本数
unique_clusters = df['Cluster'].unique()  # 获取所有唯一的簇标签（包括-1表示噪声）
median_80_dict = {col: [] for col in columns_to_cluster}  # 存储每列的80%中位值
median_80_dict['Cluster'] = []  # 存储簇标签
median_80_dict['Cluster_Size'] = []  # 存储每个簇的样本数

for cluster in unique_clusters:
    cluster_data = df[df['Cluster'] == cluster]  # 筛选当前簇的数据
    cluster_size = len(cluster_data)  # 计算当前簇的样本数
    median_80_dict['Cluster'].append(cluster)  # 添加簇标签
    median_80_dict['Cluster_Size'].append(cluster_size)  # 添加样本数
    for column in columns_to_cluster:
        if column in df.columns:
            # 对当前簇的列数据排序并取前80%
            sorted_data = np.sort(cluster_data[column].dropna())
            cutoff_index = int(len(sorted_data) * 0.8)  # 计算80%的位置
            data_80_percent = sorted_data[:cutoff_index]  # 取前80%数据
            median_80 = np.median(data_80_percent) if len(data_80_percent) > 0 else np.nan
            median_80_dict[column].append(median_80)
        else:
            print(f"警告：列 '{column}' 不存在于数据中，已跳过。")
            median_80_dict[column].append(np.nan)

# 将结果转换为DataFrame，以簇为行
median_80_df = pd.DataFrame(median_80_dict)
median_80_df.set_index('Cluster', inplace=True)

# 保存80%中位值和样本数结果到新文件
median_output_path = r'xxxx'
median_80_df.to_csv(median_output_path, encoding='utf-8-sig')
print(f"每个簇的80%中位值和样本数已保存到 {median_output_path}")

# 输出80%中位值和样本数结果查看
print("\n每个簇的80%中位值和样本数：")
print(median_80_df)
