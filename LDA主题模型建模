import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from gensim import corpora, models
from wordcloud import WordCloud

# ----------- 读取分词数据 -----------
df = pd.read_excel(r"xxxx")

def parse_tokenized(x):
    if isinstance(x, str):
        try:
            return eval(x)
        except:
            return []
    elif isinstance(x, list):
        return x
    else:
        return []

df['tokenized'] = df['tokenized'].apply(parse_tokenized)
texts = df['tokenized'].tolist()

# ----------- 创建词典和语料 -----------
dictionary = corpora.Dictionary(texts)
corpus = [dictionary.doc2bow(text) for text in texts]

# ----------- 训练 LDA 模型 -----------
num_topics = 7
lda_model = models.LdaModel(
    corpus=corpus,
    id2word=dictionary,
    num_topics=num_topics,
    random_state=42,
    passes=10,
    alpha='auto',
    per_word_topics=True
)

# ----------- 保存主题关键词及词频 -----------

with open(r'xxxxx', 'w', encoding='utf-8') as f:
    for topic_id in range(num_topics):
        topic_terms = lda_model.show_topic(topic_id, topn=40)
        term_list = [f"{word}({prob:.4f})" for word, prob in topic_terms]
        f.write(f"topic {topic_id + 1}: " + ", ".join(term_list) + "\n")

# ----------- 生成主题词云可视化 -----------
import matplotlib
matplotlib.rcParams['font.sans-serif'] = ['SimHei']  # 设置黑体显示中文
matplotlib.rcParams['axes.unicode_minus'] = False    # 正常显示负号

for topic_id in range(num_topics):
    topic_terms = lda_model.show_topic(topic_id, topn=30)
    word_freq = {word: prob for word, prob in topic_terms}

    wc = WordCloud(font_path="C:/Windows/Fonts/simhei.ttf", background_color="white", width=800, height=400)
    wc.generate_from_frequencies(word_freq)

    plt.figure(figsize=(8, 4))
    plt.imshow(wc, interpolation="bilinear")
    plt.axis("off")
    plt.title(f"topic {topic_id + 1} 词云", fontsize=14)
    plt.tight_layout()
    plt.savefig(rf"xxxx")
    plt.close()

# ----------- 计算文档主题分布并保存 -----------
doc_topics = [lda_model.get_document_topics(bow) for bow in corpus]

# 创建主题分布矩阵
topic_matrix = np.zeros((len(corpus), num_topics))
for doc_idx, topics in enumerate(doc_topics):
    for topic_id, prob in topics:
        topic_matrix[doc_idx, topic_id] = prob

# 创建 DataFrame
topic_columns = [f"topic_{i}" for i in range(num_topics)]
topic_df = pd.DataFrame(topic_matrix, columns=topic_columns)

# 添加 uid 或 doc_id
if 'uid' in df.columns:
    topic_df.insert(0, 'uid', df['uid'].values)
    id_col = 'uid'
else:
    topic_df.insert(0, 'doc_id', range(len(topic_df)))
    id_col = 'doc_id'

# 找出主导主题
topic_probs = topic_df.drop(columns=[id_col])
dominant_topic_idx = topic_probs.idxmax(axis=1)  # 返回列名，如 'topic_5'
dominant_topic = dominant_topic_idx.str.extract(r'topic_(\d+)')[0].astype(int)

# 添加主导主题列
topic_df.insert(1, 'dominant_topic', dominant_topic)

# 保存主题分布表
topic_df.to_excel(r"xxxx", index=False)

# ----------- 统计每个主题的文档数量 -----------
topic_counts = topic_df['dominant_topic'].value_counts().sort_index()

# 创建统计 DataFrame
topic_count_df = pd.DataFrame({
    'topic_id': topic_counts.index + 1,  # 主题编号从 1 开始显示
    'doc_count': topic_counts.values
})

# 输出查看
print("每个主题作为主导主题的文档数量：")
print(topic_count_df)

# 保存统计结果
topic_count_df.to_excel(r"xxxx", index=False)

print(f"\n✅ LDA模型已训练完成。主题关键词（含词频）、词云图及主题分布表均已保存。")
print(f"主题数量: {num_topics}")
print(f"文档数量: {len(corpus)}")
print(f"词典大小: {len(dictionary)}")
